

TODO:
    X find a way to only process news_items if they've yet to be stored (hash article titles)
    X convert store_news.py to article_model.py with model and methods
    - add logging to file upon scrape start and end
    - put on cron
    - add parsing of article pub dates to timestamps
    - add processing of headlines (chop up, store keywords)


- decide what we want to store and how
    ::Each News Item::
        - available props: url, author, title, description, publish time
        - shared props: url, title, publish time, (description, maybe)

    ::Each Headline::
        - give timestamp
        - split into individual words
        - remove undesired words
        - format as dictionary word count

- database and schema

- scrape scripts for news APIs
    - write simple scripts for scrape logging to file

- crons for script execution
    - write cron.bash for cron script batching
    - perhaps this needs to go into a bash deploy file for setting the cron?


notes about scraping/programmatic web requests:
    - throttle rate of requests
    - rotate IPs (look into VPNs, shared proxies and TOR)
    - user-agent string spoofing



