
TODO:
    X find a way to only process articles if they've yet to be stored (hash article titles)
    X convert store_news.py to article_model.py with model and methods
    X add logging to file upon scrape start and end
    X put on cron
    - add parsing of article publish dates to timestamps
    - add processing of headlines (chop up, store words)
    X compose schema and add table for headline words
    - refactor article hashes to concat publish date (or scrape date month) and title
    - add word_blacklist
    - make sure all storage of headline_words is lowercase
    - 


Sample query for
- select * from headline_words where scrape_ts between 'ts_int' and 'ts_int';


- decide what we want to store and how
    ::Each News Item::
        - available props: url, author, title, description, publish time
        - shared props: url, title, publish time, (description, maybe)

    ::Each Headline::
        - give timestamp
        - split into individual words
        - remove undesired words
        - format as dictionary word count


- database and schema
    article table schema:
        - id, url, author, title, description, scrape_ts, publish_ts, md5hash
    headline_word schema:
        - id, word (not unique), article_id, scrape_ts


notes about scraping/programmatic web requests:
    X throttle rate of requests
    - rotate IPs (look into VPNs, shared proxies and TOR)
    X user-agent string spoofing



